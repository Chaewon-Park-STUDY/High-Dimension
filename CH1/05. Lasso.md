## ğŸ“Œ Lasso
: Least Absolute Shrinkage and Selection Operator

### âœ… Definition
**1ï¸âƒ£ Constraint Form**
<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/7.png" width="500"/>


**2ï¸âƒ£ Penalty Form**
<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/8.png" width="400"/>
- Î» â‰¥ 0 controls amount of shrinkage
- Lasso leads to **âœ¨sparsityâœ¨** in Î²(exact zeros)
* Sparsity
=> Among many Î²s, only a few components are 'non-zero'
- t and Î» has same role
  ; Small t means strong constraint on the coefficients, which is equivalent to using a large Î». Large t implies weak constraint, which corresponds to a small Î».
<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/9.jpeg" width="400"/>
<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/11.png" width="300"/>
- small estimation error implies better estimator; meaning Î²Ì‚  well estimate Î²
<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/10.jpeg" width="450"/>

* In high dimension, our goal is to obtain **small estimation error** & **accurate support set**

- Often used for feature selection(variable selection) when p>>n
  
âœ” Advantage of Lasso: Lasso provides zero solutions for Î² ; underlying parameter( Î²) is sparse, Lasso provides accurate solution

### âœ… Lasso parameter
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/16.png" width="500"/>

- The first term is the MSE
- The second term is the L1 penalty, constrained part
- When Î»=0 , no penalty is applied
- When Î»-> âˆ, the penalty dominates, forcing all Î²â±¼ = 0
The model becomes a constant predictor: Å·áµ¢ = Î± = È³ where, Î± is the intercept and it is not penalized. 
minimize over Î±: Â Â Â âˆ‘áµ¢ (yáµ¢ - Î±)Â²
Take derivative: Â Â Â Â d/dÎ± âˆ‘áµ¢ (yáµ¢ - Î±)Â² = 0 Â Â â‡’ Â Â Î± = È³


<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/17.jpeg" width="850"/>

### âœ… Is Lasso Robust?
Things to consider;
<br>

Will the variable selection change every time the data changes?

Depending on the train and test data, the result of variable selection may differ.
How much will it differ?
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/18.png" width="600"/>
<br>
- If the boxplot is skewed--> Estimated beta hat do not change much even when the data changes(Estimated Î²1)
- Estimated Î²3, Î²4, Î²6, and Î²7 remains zero regardless of the data--> Not important variables

Lasso is Robust! 
<br>

### Lasso python code
```python
from sklearn.linear_model import Lasso

model_name = "lasso"
alpha = 1
fig = plt.figure(figsize=(6, 3))
ax = fig.add_subplot(111)
lasso = Lasso(alpha=alpha)
lasso.fit(X_train, y_train)
y_pred = lasso.predict(X_test)
rmse = np.round(np.sqrt(mean_squared_error(y_test, y_pred)), 3)
coef = pd.Series(data=lasso.coef_, index=X_train.columns).sort_values()
ax.bar(coef.index, coef.values)
ax.set_xticklabels(coef.index, rotation=90)
ax.set_title("{0}: alpha = {1}, rmse = {2}".format(model_name, alpha, rmse))
```



### âœ… Lasso Solution path
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/12.png" width="1200"/>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/13.jpeg" width="500"/>
<br>

- If the data points are far from the OLS estimator Î²Ì‚ , then the least squares value becomes large(since OLS estimator provides the smallest value for the least sqaure value)
but the L1 norm becomes small because the contour is closer to the origin (0, 0)
-  All points on the same contour have the same least squares value
-  Heuristically, the L1 constraint in Lasso has a sharp corner at the axes, so the solution tends to lie on the axes more often.
  ; meaning some coefficients are exactly zero. This is why Lasso tends to produce zero solutions.
<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/14.jpeg" width="400"/>

### âœ… Lasso penalty function
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/15.png" width="400"/>

### âœ… Lasso solutions
$\hat{\beta}^{lasso} = \arg\min_{\beta} \Big( \sum_{i=1}^{n} (y_i - x_i^\top \beta)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \Big)$

- Lasso formulation does not have a closed-form solution (because the Lâ‚-norm is not differentiable)


### âœ… (Variable) Feature Selection Property
- Under some appropriate  Î» value, some coefficients forced to be zero
- Interpretable model, reduces dimension effectively
- Good if only a few features truly matter
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/19.png" width="400"/>








---


* ì°¸ê³ ìë£Œ:  
- https://www.youtube.com/watch?v=sGTWFCq5OKM
- https://www.youtube.com/watch?v=Zb-6ZhT2zBE
