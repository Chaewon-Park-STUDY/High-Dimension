## üìå Lasso
: Least Absolute Shrinkage and Selection Operator

### ‚úÖ Definition
**1Ô∏è‚É£ Constraint Form**
<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/7.png" width="500"/>


**2Ô∏è‚É£ Penalty Form**
<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/8.png" width="400"/>
- Œª ‚â• 0 controls amount of shrinkage
- Lasso leads to **‚ú®sparsity‚ú®** in Œ≤(exact zeros)
* Sparsity
=> Among many Œ≤s, only a few components are 'non-zero'
- t and Œª has same role
  ; Small t means strong constraint on the coefficients, which is equivalent to using a large Œª. Large t implies weak constraint, which corresponds to a small Œª.
<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/9.jpeg" width="400"/>
<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/11.png" width="300"/>
- small estimation error implies better estimator; meaning Œ≤ÃÇ  well estimate Œ≤
<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/10.jpeg" width="450"/>

* In high dimension, our goal is to obtain **small estimation error** & **accurate support set**

- Often used for feature selection(variable selection) when p>>n
  
‚úî Advantage of Lasso: Lasso provides zero solutions for Œ≤ ; underlying parameter( Œ≤) is sparse, Lasso provides accurate solution

### ‚úÖ Lasso parameter
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/16.png" width="500"/>

### Lasso python code
```python
from sklearn.linear_model import Lasso

model_name = "lasso"
alpha = 1
fig = plt.figure(figsize=(6, 3))
ax = fig.add_subplot(111)
lasso = Lasso(alpha=alpha)
lasso.fit(X_train, y_train)
y_pred = lasso.predict(X_test)
rmse = np.round(np.sqrt(mean_squared_error(y_test, y_pred)), 3)
coef = pd.Series(data=lasso.coef_, index=X_train.columns).sort_values()
ax.bar(coef.index, coef.values)
ax.set_xticklabels(coef.index, rotation=90)
ax.set_title("{0}: alpha = {1}, rmse = {2}".format(model_name, alpha, rmse))
```



### ‚úÖ Lasso Solution path
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/12.png" width="1200"/>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/13.jpeg" width="500"/>
<br>

- If the data points are far from the OLS estimator Œ≤ÃÇ , then the least squares value becomes large(since OLS estimator provides the smallest value for the least sqaure value)
but the L1 norm becomes small because the contour is closer to the origin (0, 0)
-  All points on the same contour have the same least squares value
-  Heuristically, the L1 constraint in Lasso has a sharp corner at the axes, so the solution tends to lie on the axes more often.
  ; meaning some coefficients are exactly zero. This is why Lasso tends to produce zero solutions.
<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/14.jpeg" width="400"/>

### ‚úÖ Lasso penalty function
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/15.png" width="400"/>

### ‚úÖ Lasso solutions
$\hat{\beta}^{lasso} = \arg\min_{\beta} \Big( \sum_{i=1}^{n} (y_i - x_i^\top \beta)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \Big)$

- Lasso formulation does not have a closed-form solution (because the L‚ÇÅ-norm is not differentiable)














---


* Ï∞∏Í≥†ÏûêÎ£å:  
- https://www.youtube.com/watch?v=sGTWFCq5OKM
- https://www.youtube.com/watch?v=Zb-6ZhT2zBE
