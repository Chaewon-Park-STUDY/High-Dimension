## üìå What is a Good Model?
A good statistical model should satisfy the following two major criteria:

### ‚úÖ 1. Good Explanatory Model  
A model that fits the **training data** well by minimizing the error.

- Goal: Minimize the **training error**
- MSE<sub>(training)</sub> = (1/n)*(Y ‚àí ≈∂)¬≤= (1/n) * Œ£ (y·µ¢ - ≈∑·µ¢)¬≤
- This ensures the model **explains the current data accurately**.

### ‚úÖ 2. Good Predictive Model  
A model that performs well not only on the training data but also on **test data**.

- Goal: Minimize the **test error**
- This measures the model‚Äôs ability to **generalize** to new data.
- Prevents **overfitting** to training data.

#### Bias-Variance Trade off
Let $$\hat{f}(x)$$ be the fitted model.
![Bias-Variance Trade off](https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/1.png)
![Bias-Variance Trade off](https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/2.jpeg)

- In high-dimensional settings (p>>n), models can suffer from **high variance**
  --> lead to unstable predictions and poor generalization.

- To mitigate this, **regularization** is used to reduce variance, even if it introduces some bias.
- As long as the reduction in variance outweighs the increase in bias, the model‚Äôs predictive performance improves.
<br>


üéØ Graphical understanding
<br>
![Graphical definition](https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/3.png)

- The red dot represents the true value, while the blue dots represent predictions on test data.
- When predictions are close to the red dot, bias is low. When predictions are tightly clustered together, variance is low.

![Graphical definition](https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/4.png)
<br>
### ‚úÖ Motivation for Regularization
<ul>

<li>
<b>Trade-off:</b> Bias-Variance. Adding a small bias reduces variance
</li>

<li>
<b>Penalties:</b> L1 (Lasso), L2 (Ridge), both (Elastic Net: Linear Combination of Lasso & Ridge)<br>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/5.png" width="800"/>
</li>

<li>
<b>Improves Generalization:</b> Allows the model to perform well on unseen test data & Prevents overfitting especially in complex models that are sensitive to noise and fit only the training data.
</li>

</ul>
<img src="https://raw.githubusercontent.com/Chaewon-Park-STUDY/High-Dimension/main/images/6.jpeg" width="500"/>

### ‚úÖ Types of Penalties
<img src="https://github.com/user-attachments/assets/c4fd8d01-a0be-4bfb-be7c-d3aee9a5492b" width="500"/>



---






* Ï∞∏Í≥†ÏûêÎ£å:  
-https://www.youtube.com/watch?v=pJCcGK5omhE  
-https://scott.fortmann-roe.com/docs/BiasVariance.html
